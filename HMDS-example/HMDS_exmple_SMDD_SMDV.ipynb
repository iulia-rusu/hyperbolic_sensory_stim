{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb521f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method developed by Anoop Praturu: https://www.biorxiv.org/content/10.1101/2022.10.12.511940v1\n",
    "Code from Anoop Praturu on Oct 2021\n",
    "Move from pystan to cmdstanpy based on suggestions by Milo Julis\n",
    "Edited by Mingchen Yao on May 26 2023\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import diptest  \n",
    "import matplotlib.pyplot as plt\n",
    "import cmdstanpy as stan\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "from matplotlib import rcParams\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if there is anything wrong with comstan: re-install it. Otherwise don't run this cell\n",
    "# from cmdstanpy import install_cmdstan\n",
    "# install_cmdstan(overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c280a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f95b0f",
   "metadata": {},
   "source": [
    "# Functions for Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16c0781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns hyperbolic distance between vectors in poincare ball\n",
    "def poincare_dist(v1, v2):\n",
    "    sq = np.sum(np.square(v1-v2))\n",
    "    r1 = np.sum(np.square(v1))\n",
    "    r2 = np.sum(np.square(v2))\n",
    "    inv = 2.0*sq/((1.0-r1)*(1.0-r2))\n",
    "    return np.arccosh(1.0 + inv)\n",
    "\n",
    "#return NxN symmetric distance matrix from poincare coordinates\n",
    "def get_dmat(p_coords):\n",
    "    N = p_coords.shape[0]\n",
    "    dists = np.zeros((N, N))\n",
    "    \n",
    "    for i in np.arange(N):\n",
    "        for j in np.arange(i+1, N):\n",
    "            dists[i][j] = poincare_dist(p_coords[i], p_coords[j])\n",
    "            dists[j][i] = dists[i][j]\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 100 data points uniformly distributed in 5 dimensional hyperbolic space of radius R=4\n",
    "dim=5; R=4.0;\n",
    "n=100\n",
    "dirs = stats.norm.rvs(size=(n, dim))\n",
    "dirs = (dirs.T/np.sqrt(np.sum(np.square(dirs), axis=1))).T\n",
    "U = stats.uniform.rvs(size=n)\n",
    "rs_p = np.tanh(np.log((1-np.exp(-R))/np.exp(-R)*(U) + 1.0)/2.0)\n",
    "p_coords = rs_p.reshape(-1,1)*dirs\n",
    "\n",
    "#add noise to the computed distance matrix to simulate a more realistic dataset\n",
    "mat_dim = get_dmat(p_coords) + 0.05*R*stats.norm.rvs(size=(n,n))\n",
    "for i in np.arange(n):\n",
    "    for j in np.arange(i+1, n):\n",
    "        mat_dim[j][i] = mat_dim[i][j]\n",
    "mat_dim = 2.0*mat_dim/np.max(mat_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf7bb38",
   "metadata": {},
   "source": [
    "# Code for fitting Bayesian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7b4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/iuliarusu/Documents/Sharpee/HMDS-example/model/'\n",
    "ltz_m = stan.CmdStanModel(stan_file=path+'lorentz.stan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2a559",
   "metadata": {},
   "source": [
    "## Run the optimizer given the synthetically generated Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ecd94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import AVA data from all cells\n",
    "SMDD_0_df = pd.read_csv ('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDD_0_df.csv')\n",
    "SMDD_1_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDD_1_df.csv') \n",
    "SMDD_2_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDD_2_df.csv')\n",
    "SMDD_3_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDD_3_df.csv') \n",
    "SMDD_4_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDD_4_df.csv') \n",
    "SMDD_5_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDD_5_df.csv') \n",
    "SMDD_6_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDD_6_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b998505",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMDV_0_df = pd.read_csv ('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDV_0_df.csv')\n",
    "SMDV_1_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDV_1_df.csv') \n",
    "SMDV_2_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDV_2_df.csv')\n",
    "SMDV_3_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDV_3_df.csv') \n",
    "SMDV_4_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDV_4_df.csv') \n",
    "SMDV_5_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDV_5_df.csv') \n",
    "SMDV_6_df = pd.read_csv('/Users/iuliarusu/Documents/Sharpee/Clustering/clustered_bacterial_stim/SMDV_6_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process all AVA cells with from the same stim, stim groups: 0, 1, 6 OR 2, 3, 4, 5\n",
    "#stim 1\n",
    "SMDD_0 = np.array(SMDD_0_df.iloc[:, 1: -2])\n",
    "SMDD_1 = np.array(SMDD_1_df.iloc[:, 1: -3]) #this has an extra column for each cell's original id\n",
    "SMDD_6 = np.array(SMDD_6_df.iloc[:, 1: -3]) #this has an extra column for each cell's original id\n",
    "\n",
    "#concatenate by row\n",
    "SMDD_stim_0 = np.concatenate((SMDD_0, SMDD_1, SMDD_6 ), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for SMDV\n",
    "SMDV_0 = np.array(SMDV_0_df.iloc[:, 1: -2])\n",
    "SMDV_1 = np.array(SMDV_1_df.iloc[:, 1: -3]) #this has an extra column for each cell's original id\n",
    "SMDV_6 = np.array(SMDV_2_df.iloc[:, 1: -3]) #this has an extra column for each cell's original id\n",
    "\n",
    "SMDV_stim_0 = np.concatenate((SMDV_0, SMDV_1, SMDV_6 ), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a928e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate AVA and SMDV\n",
    "all_stim_0 = np.concatenate((SMDD_stim_0, SMDV_stim_0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13add130",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stim_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stim_0_df = pd.DataFrame(all_stim_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76fbba",
   "metadata": {},
   "source": [
    "# For processing both stimuli at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ae8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMDD_0 = np.array(SMDD_0_df.iloc[:, 1: -2])\n",
    "SMDD_1 = np.array(SMDD_1_df.iloc[:, 1: -3]) \n",
    "SMDD_2 = np.array(SMDD_2_df.iloc[:, 1: -3])\n",
    "SMDD_3 = np.array(SMDD_3_df.iloc[:, 1: -3])\n",
    "SMDD_4 = np.array(SMDD_4_df.iloc[:, 1: -3])\n",
    "SMDD_5 = np.array(SMDD_5_df.iloc[:, 1: -3])\n",
    "SMDD_6 = np.array(SMDD_6_df.iloc[:, 1: -3]) \n",
    "\n",
    "#concatenate by row\n",
    "SMDD_all_stim = np.concatenate((SMDD_0, SMDD_1, SMDD_2, SMDD_3, SMDD_4, SMDD_5, SMDD_6 ), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6d5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for SMDV\n",
    "SMDV_0 = np.array(SMDV_0_df.iloc[:, 1: -2])\n",
    "SMDV_1 = np.array(SMDV_1_df.iloc[:, 1: -3])\n",
    "SMDV_2 = np.array(SMDV_2_df.iloc[:, 1: -3])\n",
    "SMDV_3 = np.array(SMDV_3_df.iloc[:, 1: -3])\n",
    "SMDV_4 = np.array(SMDV_4_df.iloc[:, 1: -3])\n",
    "SMDV_5 = np.array(SMDV_5_df.iloc[:, 1: -3]) \n",
    "SMDV_6 = np.array(SMDV_2_df.iloc[:, 1: -3]) \n",
    "\n",
    "SMDV_all_stim = np.concatenate((SMDV_0, SMDV_1, SMDV_2, SMDV_3, SMDV_4, SMDV_5, SMDV_6 ), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74c0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate AVA and SMDV\n",
    "all_stim = np.concatenate((SMDD_all_stim, SMDV_all_stim), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e3e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stim_df = pd.DataFrame(all_stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "963960d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = all_stim_df.T.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028ea7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = 1 - corr_matrix\n",
    "distance_matrix_squaredp = (1 - corr_matrix**2) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "775084ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix_squaredp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db4cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:48:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "09:48:27 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "#dictionary to specify input to simulation\n",
    "\n",
    "# dat={'N':100, 'D':5, 'deltaij':mat_dim}\n",
    "dat = {'N': 26 , 'D': 3 , 'deltaij':distance_matrix_squaredp}\n",
    "#run optimizer\n",
    "model = ltz_m.optimize(data=dat, iter=250000, algorithm='LBFGS', tol_rel_grad=1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a880a1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:49:34 - cmdstanpy - WARNING - The default behavior of CmdStanMLE.stan_variable() will change in a future release to always return a numpy.ndarray, even for scalar variables.\n"
     ]
    }
   ],
   "source": [
    "# build result \n",
    "hyp_emb = {'euc':model.euc, 'sig':model.sig, 'lambda':model.stan_variable('lambda')}\n",
    "\n",
    "# # and save\n",
    "# fdname = './emb5d.pickle'\n",
    "# with open(fdname,'wb') as file:\n",
    "#     pickle.dump(hyp_emb, file,  protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load existing results\n",
    "# fdname = './emb5d.pickle'\n",
    "# with open(fdname, 'rb') as file:\n",
    "#     hyp_emb = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a4cf9",
   "metadata": {},
   "source": [
    "## Some utility functions for post-processing the simulation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f033e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_lor(t1, t2, E1, E2):\n",
    "    return np.arccosh(t1*t2 - np.dot(E1, E2))\n",
    "\n",
    "#returns embedding distance matrix from optimization fit\n",
    "def get_embed_dmat(fit):\n",
    "    N = fit['euc'].shape[0]\n",
    "    fit_ts = np.sqrt(1.0 + np.sum(np.square(fit['euc']), axis=1))\n",
    "\n",
    "    fit_mat = np.zeros((N, N))\n",
    "\n",
    "    for i in np.arange(N):\n",
    "        for j in np.arange(i+1,N):\n",
    "            fit_mat[i][j] = d_lor(fit_ts[i], fit_ts[j], fit['euc'][i], fit['euc'][j])\n",
    "            fit_mat[j][i] = fit_mat[i][j]\n",
    "            \n",
    "    return fit_mat\n",
    "\n",
    "#return poincare coordinates\n",
    "def get_poin(fit):\n",
    "    ts = np.sqrt(1.0 + np.sum(np.square(fit['euc']), axis=1))\n",
    "    return (fit['euc'].T / (ts + 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dac6394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sim(fit):\n",
    "    fit['emb_mat'] = get_embed_dmat(fit)/fit['lambda']\n",
    "    fit['pcoords'] = get_poin(fit)\n",
    "    fit['radii'] = 2.0*np.arctanh(np.sqrt(np.sum(np.square(fit['pcoords']), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d651f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this function after running the optimizer to process the output data into more usable forms\n",
    "process_sim(hyp_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d3b4c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.41017"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#curvature (or radius)\n",
    "hyp_emb['lambda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa441edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51096545,  0.58419151, -0.62713135],\n",
       "       [-0.69071447,  0.46324387,  0.55034174],\n",
       "       [-0.67776413,  0.48921421,  0.54402729],\n",
       "       [ 0.99018207, -0.05515661, -0.10810333],\n",
       "       [ 0.98883649, -0.05561645, -0.11819745],\n",
       "       [ 0.46298783, -0.87807918,  0.09669888],\n",
       "       [ 0.45803423, -0.88030509,  0.09960221],\n",
       "       [ 0.43358292,  0.07117845,  0.89572109],\n",
       "       [-0.53635098,  0.65370638, -0.52935122],\n",
       "       [-0.54174629,  0.65730059, -0.51939034],\n",
       "       [ 0.29034882,  0.95317333,  0.05017934],\n",
       "       [ 0.28748694,  0.95463114,  0.03560646],\n",
       "       [-0.7465279 , -0.59032691, -0.29897306],\n",
       "       [-0.76728328, -0.57949622, -0.26606726],\n",
       "       [-0.80281736,  0.02150933,  0.59134841],\n",
       "       [-0.80949993,  0.00660463,  0.58294756],\n",
       "       [-0.82776005,  0.03847497,  0.55491822],\n",
       "       [ 0.19390924, -0.35210191, -0.91266043],\n",
       "       [ 0.19243721, -0.34288982, -0.91645853],\n",
       "       [ 0.03848929, -0.91650764,  0.39193028],\n",
       "       [ 0.01105581, -0.91160352,  0.40515032],\n",
       "       [ 0.50711341,  0.31716627,  0.79842517],\n",
       "       [ 0.606664  ,  0.1621767 ,  0.7750512 ],\n",
       "       [-0.31948977,  0.33496144, -0.88326426],\n",
       "       [ 0.19410969, -0.35205555, -0.91263575],\n",
       "       [ 0.19251902, -0.34287182, -0.91644702]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#poincare embedding coordinates\n",
    "hyp_emb['pcoords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "353742ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'SMDD_SMDV_pcoords_sphere' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "SMDD_SMDV_pcoords_sphere = hyp_emb['pcoords']\n",
    "%store SMDD_SMDV_pcoords_sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMDD_SMDV_pcoords_15 = hyp_emb['pcoords']\n",
    "%store SMDD_SMDV_pcoords_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emebdding uncertainties\n",
    "hyp_emb.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4a0f2",
   "metadata": {},
   "source": [
    "# Shepard Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68aec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,7.5))\n",
    "\n",
    "ax[0].scatter(distance_matrix_squaredp.values[np.triu_indices(12, k=1)], hyp_emb['emb_mat'][np.triu_indices(12, k=1)], c='cornflowerblue') #cornflowerblue, lightcoral\n",
    "ax[0].plot(np.arange(3), np.arange(3), c='black', linewidth=5, alpha=0.5)\n",
    "\n",
    "ax[0].set_xlabel('Original Distances', fontsize=20)\n",
    "ax[0].set_ylabel('Embedding Distances / $\\lambda$', fontsize=20)\n",
    "\n",
    "ax[1].hist(hyp_emb['sig'], color ='cornflowerblue') #cornflowerblue, lightcoral\n",
    "ax[1].set_xlabel('Embedding Uncertainties', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea16b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperbolic_df= pd.DataFrame(hyp_emb['pcoords'], columns=['x', 'y', 'z'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d435bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['SMDD'] * (SMDD_0.shape[0] + SMDD_1.shape[0] + SMDD_2.shape[0] + SMDD_3.shape[0] + SMDD_4.shape[0] + SMDD_5.shape[0] + SMDD_6.shape[0]) + ['SMDV'] * (SMDV_0.shape[0] + SMDV_1.shape[0] + SMDV_2.shape[0] + SMDV_3.shape[0] + SMDV_4.shape[0] + SMDV_5.shape[0] + SMDV_6.shape[0] )\n",
    "SMDD_SMDV_labels = np.array(column)\n",
    "hyperbolic_df['cluster labels'] = SMDD_SMDV_labels.T\n",
    "hyperbolic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ae8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store SMDD_SMDV_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a scatter plot\n",
    "# fig = px.scatter_3d(data_frame = hyperbolic_df, x= 'x', y = 'y', z= 'z', title='SMDD and SMDV: Stimulus 0', color = 'cluster labels') #cornflowerblue, lightcoral\n",
    "\n",
    "fig = px.scatter_3d(data_frame = hyperbolic_df, x= 'x', y = 'y', z= 'z', title='SMDD and SMDV Neurons: Bacterial Stimulus No. 0',  color ='cluster labels') #cornflowerblue, lightcoral\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(ticktext=[-1, -0.5, 0, 0.5, 1]),\n",
    "        yaxis=dict(ticktext=[-1, -0.5, 0, 0.5, 1]),\n",
    "        zaxis=dict(ticktext=[-1, -0.5, 0, 0.5, 1]),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90339bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "\n",
    "# Example DataFrame, assuming hyperbolic_df is already defined with appropriate data\n",
    "# Adding a column 'labels' to hyperbolic_df for the point labels if not already present\n",
    "# hyperbolic_df['labels'] = ...\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    data_frame=hyperbolic_df, \n",
    "    x='x', \n",
    "    y='y', \n",
    "    z='z', \n",
    "    color='cluster labels', \n",
    "    # text='cluster labels',  # assuming 'labels' column contains the point labels\n",
    "    # title='AVA and RME Neurons',\n",
    "    labels={'x': 'X', 'y': 'Y', 'z': 'Z'},\n",
    "    color_discrete_map={'SMDD': 'yellow', 'SMDV':'purple'}\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            tickvals=[-1, -0.5, 0, 0.5, 1],\n",
    "            ticktext=['-1', '-0.5', '0', '0.5', '1'],\n",
    "            title=dict(text='X', font=dict(size=20)),\n",
    "            tickfont=dict(size=15),\n",
    "            showgrid=False\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            tickvals=[-1, -0.5, 0, 0.5, 1],\n",
    "            ticktext=['-1', '-0.5', '0', '0.5', '1'],\n",
    "            title=dict(text='Y', font=dict(size=20)),\n",
    "            tickfont=dict(size=15),\n",
    "            showgrid=False\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            tickvals=[-1, -0.5, 0, 0.5, 1],\n",
    "            ticktext=['-1', '-0.5', '0', '0.5', '1'],\n",
    "            title=dict(text='Z', font=dict(size=20)),\n",
    "            tickfont=dict(size=15),\n",
    "            showgrid=False\n",
    "        )\n",
    "    ),\n",
    "    legend=dict(\n",
    "        title=dict(text='Cluster Labels', font=dict(size=12)),\n",
    "        font=dict(size=10)\n",
    "    ),\n",
    "    # title=dict(\n",
    "    #     text='SMDD and SMDV Neurons',\n",
    "    #     font=dict(size=15)\n",
    "    # ),\n",
    "    margin=dict(l=20, r=20, b=90, t=40),\n",
    "    autosize=True,\n",
    "    width=700,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "# Customize marker size and opacity for better visualization\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.6))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"SMDD_SMDV.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6080734",
   "metadata": {},
   "source": [
    "# Fitting For Dimension\n",
    "\n",
    "Suppose we did not know a-priori that the data was 5D? This is usually the case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e919b",
   "metadata": {},
   "source": [
    "## Fit the model across a range of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffeb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fits = []\n",
    "for d in np.arange(2,15):\n",
    "    dat={'N':26, 'D':d, 'deltaij':distance_matrix_squaredp}\n",
    "    #run optimizer\n",
    "    model = ltz_m.optimize(data=dat, iter=250000, algorithm='LBFGS', tol_rel_grad=1e2)\n",
    "    all_fits.append({'euc':model.euc, 'sig':model.sig, 'lambda':model.stan_variable('lambda'), 't':model.time})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ddd178",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.stan_variable('lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e53bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access the correct iteration \n",
    "iteration = all_fits[5]['t']\n",
    "\n",
    "\n",
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate radii based on time parameter\n",
    "radii = np.arccosh(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of radii\n",
    "fig, ax = plt.subplots(figsize=(15,7.5))\n",
    "\n",
    "ax.hist(radii, color ='cornflowerblue') #cornflowerblue, lightcoral\n",
    "ax.set_xlabel('Hyperbolic Embedding Radii', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c766bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIP analysis of radii\n",
    "dip, pval = diptest.diptest(radii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac64e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dip, pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b674d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fits[0]['sig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return negative log likelihood of fit\n",
    "def MDS_lkl(fit, dmat):\n",
    "    lkl = 0;\n",
    "    N = fit['sig'].shape[0]\n",
    "    \n",
    "    sigs = fit['sig']\n",
    "    lam = fit['lambda']\n",
    "    emb_mat = get_embed_dmat(fit)\n",
    "    \n",
    "    for i in np.arange(N):\n",
    "        for j in np.arange(i+1, N):\n",
    "            seff = sigs[i]**2 + sigs[j]**2\n",
    "            lkl += ((dmat[i][j] - emb_mat[i][j]/lam)**2 / (2.0*seff)) + 0.5*np.log(seff*2.0*np.pi)\n",
    "    return lkl\n",
    "\n",
    "#input: optimization fit and distance matrix\n",
    "def BIC(fit, dmat):\n",
    "    N,D = fit['euc'].shape\n",
    "    n = 0.5*N*(N-1)\n",
    "    k = N*D + N + 1.0 - 0.5*D*(D-1)\n",
    "    \n",
    "    return k*np.log(n) + 2.0*MDS_lkl(fit, dmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccab40",
   "metadata": {},
   "source": [
    "## Bayesian information criteria\n",
    "\n",
    "There is a lot of formal Bayesian Theory behind this (see chapter 2 here https://urldefense.proofpoint.com/v2/url?u=https-3A__www.inference.org.uk_mackay_thesis.pdf&d=DwIGAg&c=-35OiAkTchMrZOngvJPOeA&r=B8GeUuyHfxQP8MseZuhipQ&m=KVww4gh9-XOtp1LqNUc0K-PGXOX3bm2QsokPFlBG9Vs&s=UBtNEdIXatq_zFpG53nmPCLbCnlgIgWigHYhEnBRYyo&e= ), but essentially we are trying to find the minimal number of parameters to describe a dataset. If the data is 5D, we don't want to use 7 parameters to describe it. The BIC is like a cost function that rewards a model that has a better fit to the likelihood function, but penalizes models that increase the number of their parameters. The model which minimizes this function will thus have the ideal trade-off of being able to model the data well without introducing too many parameters and overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3688551",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_BIC = [BIC(fit, distance_matrix_squaredp) for fit in all_fits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1160cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae8d10",
   "metadata": {},
   "source": [
    "As you can see the BIC is minimized at the true dimension of 5. Any more parameters would have been redundant, any less would not have properly fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3160255",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7.5,7.5))\n",
    "\n",
    "ax.scatter(np.arange(2,15), all_BIC, c = 'cornflowerblue') #cornflowerblue, lightcoral\n",
    "ax.set_xlabel('Dimension', fontsize=20)\n",
    "ax.set_ylabel('BIC', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce171280",
   "metadata": {},
   "source": [
    "# Re-Running the optimizer\n",
    "Lets start by generating some 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 100 data points uniformly distributed in 5 dimensional hyperbolic space of radius R=4\n",
    "dim=2; R=4.0;\n",
    "n=100\n",
    "dirs = stats.norm.rvs(size=(n, dim))\n",
    "dirs = (dirs.T/np.sqrt(np.sum(np.square(dirs), axis=1))).T\n",
    "U = stats.uniform.rvs(size=n)\n",
    "rs_p = np.tanh(np.log((1-np.exp(-R))/np.exp(-R)*(U) + 1.0)/2.0)\n",
    "p_coords = rs_p.reshape(-1,1)*dirs\n",
    "\n",
    "#add noise to the computed distance matrix to simulate a more realistic dataset\n",
    "mat_2D = get_dmat(p_coords) + 0.05*R*stats.norm.rvs(size=(n,n))\n",
    "for i in np.arange(n):\n",
    "    for j in np.arange(i+1, n):\n",
    "        mat_2D[j][i] = mat_2D[i][j]\n",
    "mat_2D = 2.0*mat_2D/np.max(mat_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to specify input to simulation\n",
    "dat={'N':100, 'D':2, 'deltaij':mat_2D}\n",
    "#run optimizer\n",
    "model2D = ltz_m.optimize(data=dat, iter=250000, algorithm='LBFGS', tol_rel_grad=1e2)\n",
    "hyp_emb2D = {'euc':model2D.euc, 'sig':model2D.sig, 'lambda':model2D.stan_variable('lambda')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sim(hyp_emb2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed16b6",
   "metadata": {},
   "source": [
    "#### Most pts are well fit, but a few are poorly fit (the ones with high sigma) which adds a lot of scatter to the shepard diagram\n",
    "#### The optimizer got caught in a false minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,7.5))\n",
    "\n",
    "ax[0].scatter(mat_2D[np.triu_indices(100, k=1)], hyp_emb2D['emb_mat'][np.triu_indices(100, k=1)])\n",
    "ax[0].plot(np.arange(3), np.arange(3), c='black', linewidth=5, alpha=0.5)\n",
    "\n",
    "ax[0].set_xlabel('Original Distances', fontsize=20)\n",
    "ax[0].set_ylabel('Embedding Distances / $\\lambda$', fontsize=20)\n",
    "\n",
    "ax[1].hist(hyp_emb2D['sig'])\n",
    "ax[1].set_xlabel('Embedding Uncertainties', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edc498",
   "metadata": {},
   "source": [
    "### Strategy: randomize the positions of the poorly fit points, and then return the coordinates as the initial conditions to continue optimizing. This bumps the simulation out of the false minimium without destroying all of the work its already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47006a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out which pts have high uncertainty, and randomize their coordinates without touching the rest of the points\n",
    "N_refit = np.where(hyp_emb2D['sig'] > 0.3)[0].shape[0]\n",
    "hyp_emb2D['euc'][np.where(hyp_emb2D['sig'] > 0.3)] = stats.norm.rvs(size=(N_refit,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-reun the optimizer, but this time sepcify the initial condition by passing the previous simulation dictionary\n",
    "dat={'N':100, 'D':2, 'deltaij':mat_2D}\n",
    "model2D = ltz_m.optimize(data=dat, iter=250000, algorithm='LBFGS', tol_rel_grad=1e2,inits = hyp_emb2D)\n",
    "hyp_emb2D = {'euc':model2D.euc, 'sig':model2D.sig, 'lambda':model2D.stan_variable('lambda')}\n",
    "# tst_2D = ltz_m.optimizing(data=dat, iter=250000, tol_rel_grad=1e2, init=tst_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc911c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sim(hyp_emb2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b534e",
   "metadata": {},
   "source": [
    "### We see that the fit is significantly better, although there are still a few poorly fit points. We could keep iterating the above process until all points are well fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fa17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,7.5))\n",
    "\n",
    "ax[0].scatter(mat_2D[np.triu_indices(100, k=1)], hyp_emb2D['emb_mat'][np.triu_indices(100, k=1)])\n",
    "ax[0].plot(np.arange(3), np.arange(3), c='black', linewidth=5, alpha=0.5)\n",
    "\n",
    "ax[0].set_xlabel('Original Distances', fontsize=20)\n",
    "ax[0].set_ylabel('Embedding Distances / $\\lambda$', fontsize=20)\n",
    "\n",
    "ax[1].hist(hyp_emb2D['sig'])\n",
    "ax[1].set_xlabel('Embedding Uncertainties', fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
